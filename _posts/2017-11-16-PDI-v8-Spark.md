---
layout: post
title: "Pentaho Data Integration v8: Getting started with the Spark Execution Engine"
summary: This article explains how to configure PDI to run with Spark
date: 2017-11-16
categories: PDI
tags: PDI, Spark
published: true
---

# PDI AEL Spark Version 8 Improvements

At **Pentaho World 2017** following improvements were mentioned ([Source](http://pedroalves-bi.blogspot.co.uk/2017/10/pentaho80.html)):

- Communicates with Pentaho client tools over WebSocket; does NOT require Zookeeper
- Uses distro-specific Spark library
- Enhanced Kerberos impersonation on client-side

This brings a bunch of benefits:

- Reduced number of steps to setup 
- Enable fail-over, load-balancing
- Robust error and status reporting 
- Customization of Spark jobs (i.e. memory , settings)
- Client to AEL connection can be secured
- Kerberos impersonation from client tool 

Other improvements:

- Big Data File Formats - **Avro and Parquet** steps: When you run in AEL, these will also be natively interpreted by the engine, which adds a lot to the value of this.

Other notes:

- There is still no improvement on the Group By step. Basically, everything still has to go via a single node (Spark client) - there is no proper Spark support to run it in parallel.
- There is no support in AEL Spark Streaming for event time in a windowing function. Windowing can be achieved (as highlighted in Pedro’s blog) by using a sub-transformation (you can define the batch interval in the job entry settings) and then you can do the aggregation in the sub-transformation based on processing time (not event, not ingestion time).
- Spark client: Is there still a limitation to run it on the edgenode? Yes.

**Official Documentation**:

- Pentaho v8: [Set Up the Adaptive Execution Layer (AEL)](https://help.pentaho.com/Documentation/8.0/Setup/Configuration/Adaptive_Execution_Layer#Pentaho_Spark_Application)

# Setup for v8

## Spark Client

If not already installed on your local machine, download the **Spark Client** from [here](http://spark.apache.org/downloads.html).

The **environment variable** `SPARK_HOME` will point to the **Spark Client** folder.


## PDI

PDI Download:

- PDI-CE **v8.0** from [here](https://sourceforge.net/projects/pentaho/files/Pentaho%208.0/client-tools/pdi-ce-8.0.0.0-28.zip/download).

**Unzip** it and move the folder to a convenient location. Make the shell files **executable**. 

## Configuration

### Local Setup for Development ONLY

The **exciting news in PDI v8** is that Pentaho took all the complexity out of the local setup! It is really easy now to get going with PDI and Spark!

**AEL Daemon**: The **Adaptive Execution Layer** is part of the PDI client now! You can find it in the `adative-execution` folder within `<pdi-client-root>`.  The **environment variable** `PDI_AEL_DAEMON_HOME` will point not to this folder but the parent folder, which is `<pdi-client-root>`.

Adjust the configuration of the **AEL Daemon** by adjusting this file: 

```
data-integration/adaptive-execution/config/application.properties
```

Change the following properties:

| property | value          | environment variable 
|----------|----------------|-------------
| `sparkHome` | path your local Spark client | `SPARK_HOME`
| `sparkApp` | path to your AEL Daemon `data-integration` folder | `PDI_AEL_DAEMON_HOME`
| `hadoopConfDir` | path to directory containing all the Hadoop config details (`*-site.xml`) | `HADOOP_CONF_DIR`

Here is my example configuration with hard-coded values (import bits shown only):

```
# Hadoop Configuration
# ----------------------------------------------------------------------------------------------------
# Directory where *-site.xml files reside for Hadoop Configuration
hadoopConfDir=/home/dsteiner/apps/hadoop-2.8.0/etc/hadoop

# Hadoop User which will run job
hadoopUser=dsteiner

# Spark Configuration
# ----------------------------------------------------------------------------------------------------
# Location of Apache Spark Client distribution\
sparkHome=/home/dsteiner/apps/spark-2.1.0-bin-hadoop2.7

# Spark Master
# Where spark will run: local / yarn
# Note, you can also simulate multiple executors by using this notation (i.e. 2 executors):  local[2]
sparkMaster=local[2]

# Deploy Mode
# This property is only used when the master is set to `yarn`.
# client  - Driver will run on the daemon machine; but the executors will run in Yarn
# cluster - Entire application will be run in cluster (Not supported yet)
sparkDeployMode=client

# AEL Spark Properties
# ----------------------------------------------------------------------------------------------------
# Main PDI Driver location (can be pdi-ee-client or pdi-spark-driver)
sparkApp=/home/dsteiner/apps/pdi-ce-8.0
```

Alternatively, if you set the environment variables in `bashrc` (in example):

```bash
export HADOOP_CONF_HOME=<your-value>
export SPARK_HOME=<your-value>
export PDI_AEL_DAEMON_HOME=<your-value>
```

You can reference them inside the config like so:

```bash
# Hadoop Configuration
# ----------------------------------------------------------------------------------------------------
# Directory where *-site.xml files reside for Hadoop Configuration
hadoopConfDir=${HADOOP_CONF_DIR}

# Hadoop User which will run job
hadoopUser=dsteiner

# Spark Configuration
# ----------------------------------------------------------------------------------------------------
# Location of Apache Spark Client distribution\
sparkHome=${SPARK_HOME}

# Spark Master
# Where spark will run: local / yarn
# Note, you can also simulate multiple executors by using this notation (i.e. 2 executors):  local[2]
sparkMaster=local[2]

# Deploy Mode
# This property is only used when the master is set to `yarn`.
# client  - Driver will run on the daemon machine; but the executors will run in Yarn
# cluster - Entire application will be run in cluster (Not supported yet)
sparkDeployMode=client

# AEL Spark Properties
# ----------------------------------------------------------------------------------------------------
# Main PDI Driver location (can be pdi-ee-client or pdi-spark-driver)
sparkApp=${PDI_AEL_DAEMON_HOME}
```

Next create a directory called `kettleConf` inside your **Spark Client**'s root directory:

```bash
cd $SPARK_HOME
mkdir kettleConf
```

> **Note**: If the `kettleConf` folder already exists and has a few files in it, it is probably best to remove these files.

Next start the **AEL Daemon** like so:

```bash
cd $PDI_AEL_DAEMON_HOME
# run as foreground process - great for debugging
sh ./daemon.sh
# or start as a background process
sh ./daemon.sh start
# check the status if in background mode
./daemon.sh status
```

#### Create Run Configuration

Now that the **AEL Daemon** is running, we can create a **Run Configuration** in **Spoon**. You can think of this as configuring just another **engine** to run your jobs or transformations with.

Setting up a **Run Configuration** in **Spoon** is very easy:

1. Go to the **View tab**.
2. Right click on **Run configurations** and choose **New**.
3. In the **Run configuration** dialog provide a **name** and description. As **Engine** choose `Spark`. The default connection entries should be just fine for your local setup (`http://127.0.0.1:53000`).

> **Note**: You might wonder what port `53000` relates to? This is the port defined for the **AEL Deamon** in `adaptive-execution/config/application.properties`.
![Run Configuration Dialog](/images/pdi-v8-spark/pdi-v8-spark-1.png)

When you execute your job or transformation next time, pick this **Run Configuration**.

#### FAQ

- **Q**: Does the Spark client or server have to be running for PDI AEL to work? **A**: No. PDI AEL will start the Spark Client by itself.
- **Q**: Is it necessary to have Spark set up to connect to HDFS? **A**: No. If you want to run Hadoop specific steps/job entries, `sparkMaster` must be set to `yarn`. You can simulate this locally by running a pseudo-distributed Hadoop cluster. If you want to do this, configure the AEL Daemon as shown in the next section and also upload pdi-spark-executor.zip` to HDFS.
- **Q**: Is it necessary to have HDFS running? **A**: No, unless you changed the AEL Daemon config so that everything runs against YARN.

### Running on the Cluster (YARN Mode)

> **Note**: There is no dependency any more on **Zookeeper** in v8.

#### Spark Application

Since some users extend the PDI core functionality by additional steps (plugins) e.g. from the Marketplace, not every PDI setup is the same. We will have to make the essential libraries of our possibly customised PDI setup available to the Spark cluster via building an zip file which bundles any dependencies (libraries). If you've every written a plain Scala or Java Scala app and then created an uber jar, this will all sound quite familiar to you.

**PDI** comes with the **Spark Application Builder Tool**, which we will use to create this zip file (called `pdi-spark-driver.zip`). To be precise, the zip file has two essential contents:

- `data-integration`: This looks pretty much like your local PDI client installation
- `pdi-spark-executor.zip`: bundles all the required PDI libraries, so that it can be distributed on the Spark cluster.


The idea is that you copy `pdi-spark-driver.zip` to an **edgenode** on your cluster and extract it there. 

If you are just planning to do local development, you can unzip `pdi-spark-driver.zip` locally as well.

And this how your create the zip file on Linux:

```bas
cd <pdi-root>
sh ./spark-app-builder.sh -outputLocation /tmp
```

#### Configuration

If you intend to use with Spark and YARN on the cluster, the following additional steps are necessary:

Add following variables to `bashrc` (in example):

```
export HADOOP_CONF_HOME=<your-value>
export SPARK_HOME=<your-value>
export PDI_AEL_DAEMON_HOME=<your-value>
export HDFS_SPARK_EXECUTOR_LOCATION=<your-value>
```

Copy `pdi-spark-driver.zip` to the **edge node** and unzip the file. As mentioned before, you will find a `data-integration` folder and the `pdi-spark-executor.zip` inside the extracted folder.

Adjust the configuration of the **AEL Daemon** by adjusting this file: 

```
data-integration/adaptive-execution/config/application.properties
```

Change the following properties:

| property | value          | environment variable 
|----------|----------------|-------------
| `sparkHome` | path your local Spark client | `SPARK_HOME`
| `sparkApp` | path to your AEL Daemon `data-integration` folder | `PDI_AEL_DAEMON_HOME`
| `hadoopConfDir` | path to directory containing all the Hadoop config details (`*-site.xml`) | `HADOOP_CONF_DIR`
| `hadoopUser` | user id which submits the PDI Spark job. Only applicable if you are not using security |
| `sparkMaster` | `yarn` |
| assemblyZip | `hdfs:$HDFS_SPARK_EXECUTOR_LOCATION` | `HDFS_SPARK_EXECUTOR_LOCATION`

Next create a directory called `kettleConf` inside your **Spark Client**'s root directory:

```bash
cd $SPARK_HOME
mkdir kettleConf
```


Copy `pdi-spark-executor.zip` to **HDFS** and extract the contents. This location is referred to as `HDFS_SPARK_EXECUTOR_LOCATION`. 

```bash
hdfs dfs put pdi-spark-executor.zip /opt/pentaho/pdi-spark-executor.zip
```

Next start the **AEL Daemon** like so:

```bash
cd $PDI_AEL_DAEMON_HOME
sh ./daemon.sh
# or start as a background process
sh ./daemon.sh start
# check the status if in background mode
./daemon.sh status
```


## Securing AEL Spark

Take a look at the info provided [here](https://help.pentaho.com/Documentation/8.0/Setup/Configuration/Adaptive_Execution_Layer#Pentaho_Spark_Application)